{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnatEleOCkXL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOiYtURdunA3"
   },
   "source": [
    "Importing the libraries to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWPtFCZHCqVv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJc_ic7Juu9A"
   },
   "source": [
    "Using keras only to import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88mucL_aCzn0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "print(type(train_x))\n",
    "print(train_x.shape)\n",
    "print(type(train_y))\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "wVg9BWQ3upOm",
    "outputId": "bf7dbd3e-656f-431e-9463-f3372a25fc50"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    fig.add_subplot(1,6,i+1)\n",
    "    plt.imshow(train_x[i], vmin = 0, vmax = 255, cmap = 'gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying first few digits of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk7unl1ZTmhL",
    "outputId": "a7de25fa-43b0-4a06-b5da-fa9b87f90c8c"
   },
   "outputs": [],
   "source": [
    "train_x_upd = np.zeros(784*train_x.shape[0]).reshape(train_x.shape[0], 784)\n",
    "for i in range(train_x.shape[0]):\n",
    "  train_x_upd[i] = train_x[i].reshape(784)\n",
    "  train_x_upd[i] = train_x_upd[i]/256\n",
    "\n",
    "print(type(train_x_upd[1]))\n",
    "print(train_x_upd[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_upd = np.zeros(784*test_x.shape[0]).reshape(test_x.shape[0], 784)\n",
    "for i in range(test_x.shape[0]):\n",
    "  test_x_upd[i] = test_x[i].reshape(784)\n",
    "  test_x_upd[i] = test_x_upd[i]/256\n",
    "\n",
    "print(type(test_x_upd[1]))\n",
    "print(test_x_upd[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QW7n4wZu2FK"
   },
   "source": [
    "Process the data to convert it from a 28x28 array to a 784x1 array. Further, the input is normalised by dividing the values by 256, so that the input layer activations lie between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtNZlxdBG9NE",
    "outputId": "b87e5afa-c520-4f31-f2be-8c5b6aa7f7f6"
   },
   "outputs": [],
   "source": [
    "def output(x):\n",
    "  out = np.zeros(10)\n",
    "  out[x] = 1.0\n",
    "  return out\n",
    "\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "\n",
    "plt.imshow(train_x_upd[0].reshape(28,28), cmap='gray', vmin = 0, vmax = 1)\n",
    "plt.show()\n",
    "\n",
    "print(train_y[0])\n",
    "w = output(train_y[0])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qu7-EaDDOs0F"
   },
   "outputs": [],
   "source": [
    "print(train_y.shape)\n",
    "output_arr = np.zeros(10*train_y.shape[0])\n",
    "output_arr = output_arr.reshape(train_y.shape[0], 10)\n",
    "print(output_arr.shape)\n",
    "\n",
    "for i in range(train_y.shape[0]):\n",
    "  output_arr[i] = output(train_y[i])\n",
    "\n",
    "output_arr\n",
    "train_y = output_arr.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOLCpzD0vVh2"
   },
   "source": [
    "Convert the output labels into an array of size 10x1 which can be directly compared with the output layer activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djEVWRuOjnKx",
    "outputId": "3879f048-e0c8-44f5-a01f-a10cf2805b33"
   },
   "outputs": [],
   "source": [
    "def sigmoid_num(x):\n",
    "  if(x > 20):\n",
    "    return 1\n",
    "  if(x < -20):\n",
    "    return 0\n",
    "  return (1/(1 + np.exp(-x)))\n",
    "\n",
    "sigmoid = np.vectorize(sigmoid_num)\n",
    "sigmoid(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMYbB8PypiL7"
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime_num(x):\n",
    "  return ((sigmoid_num(x))*(1 - sigmoid_num(x)))\n",
    "\n",
    "sigmoid_prime = np.vectorize(sigmoid_prime_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylCalr84wFwz"
   },
   "source": [
    "The sigmoid function is the activation function. It has been vectorised to make it easy to apply on numpy arrays. Also to avoid np.exp overflow issues, sigmoid has been defines specifically for |x|>20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    #this class contains all the methods which are needed to train the network.\n",
    "    #this class also has the weights, biases and neuron activations as attributes.\n",
    "\n",
    "    def __init__(self, size_layers):\n",
    "        self.inputlayer = np.zeros(size_layers[0])\n",
    "        self.hiddenlayer = np.zeros(size_layers[1])\n",
    "        self.outputlayer = np.zeros(size_layers[2])\n",
    "\n",
    "        self.hid_z = np.zeros(size_layers[1])\n",
    "        self.out_z = np.zeros(size_layers[2])\n",
    "\n",
    "        self.out_error = np.zeros(size_layers[2])\n",
    "        self.hid_error = np.zeros(size_layers[1])\n",
    "\n",
    "        self.weights_inp_hid = np.random.normal(size = (size_layers[1],size_layers[0]))\n",
    "        self.weights_hid_out = np.random.normal(size = (size_layers[2],size_layers[1]))\n",
    "\n",
    "        self.bias_hid = np.random.normal(size = size_layers[1])\n",
    "        self.bias_out = np.random.normal(size = size_layers[2])\n",
    "\n",
    "    def feedfwd(self, a):\n",
    "        #given an input image, this method calculates the neron activations at each layer of the network.\n",
    "        self.inputlayer = a\n",
    "\n",
    "        self.hid_z = np.matmul(self.weights_inp_hid, self.inputlayer) + self.bias_hid\n",
    "        self.hiddenlayer = sigmoid(self.hid_z)\n",
    "\n",
    "        self.out_z = np.matmul(self.weights_hid_out, self.hiddenlayer) + self.bias_out\n",
    "        self.outputlayer = sigmoid(self.out_z)\n",
    "\n",
    "    def backprop(self, expected_res):\n",
    "        #this method calculates and returns the various terms of the gradient of the cost function, using the backpropagation algorithm\n",
    "        self.out_error = np.multiply(self.outputlayer - expected_res, sigmoid_prime(self.out_z))\n",
    "        self.hid_error = np.multiply(np.matmul(np.transpose(self.weights_hid_out), self.out_error), sigmoid_prime(self.hid_z))\n",
    "\n",
    "        grad_bias_out = self.out_error\n",
    "        grad_bias_hid = self.hid_error\n",
    "        \n",
    "        grad_weight_inp_hid = np.outer(self.hid_error, self.inputlayer)\n",
    "        grad_weight_hid_out = np.outer(self.out_error, self.hiddenlayer)\n",
    "\n",
    "        return grad_weight_inp_hid, grad_weight_hid_out, grad_bias_hid, grad_bias_out\n",
    "\n",
    "    def gradient_desc(self, sample, learn_rate): \n",
    "        #sample is a list of tuples (x,y) where x is the input layer values, and y is the expected output.\n",
    "        #this method calls backprop() to evaluate the gradient of the cost function. Then it averages the gradient over all inputs in sample and \n",
    "        #using this average gradient and the learn_rate, modifies the weights and biases (gradient descent)\n",
    "        avg_grad_weight_inp_hid = np.zeros(shape = self.weights_inp_hid.shape)\n",
    "        avg_grad_weight_hid_out = np.zeros(shape = self.weights_hid_out.shape)\n",
    "        avg_grad_bias_hid = np.zeros(shape = self.bias_hid.shape)\n",
    "        avg_grad_bias_out = np.zeros(shape = self.bias_out.shape)\n",
    "\n",
    "        for a in sample:\n",
    "            self.feedfwd(a[0])\n",
    "            grad_weight_inp_hid, grad_weight_hid_out, grad_bias_hid, grad_bias_out = self.backprop(a[1])\n",
    "\n",
    "            avg_grad_weight_inp_hid = (avg_grad_weight_inp_hid*i + grad_weight_inp_hid)/(i+1)\n",
    "            avg_grad_weight_hid_out = (avg_grad_weight_hid_out*i + grad_weight_hid_out)/(i+1)\n",
    "            avg_grad_bias_hid = (avg_grad_bias_hid*i + grad_bias_hid)/(i+1)\n",
    "            avg_grad_bias_out = (avg_grad_bias_out*i + grad_bias_out)/(i+1)\n",
    "\n",
    "        length = len(sample)\n",
    "\n",
    "        self.weights_inp_hid -= (learn_rate/length) * avg_grad_weight_inp_hid\n",
    "        self.weights_hid_out -= (learn_rate/length) * avg_grad_weight_hid_out\n",
    "        self.bias_hid -= (learn_rate/length) * avg_grad_bias_hid\n",
    "        self.bias_out -= (learn_rate/length) * avg_grad_bias_out\n",
    "\n",
    "    def stochastic_batch(self, train_x_upd, train_y, batch_size, epochs, learn_rate, test_x_upd, test_y):\n",
    "        #instead of taking the average over all of the inputs in the training data, we can divide the training data into random minibatches, \n",
    "        #and modify our weights and biases using the average gradient over the inputs of each minibatch.\n",
    "\n",
    "        #we do this for the given number of epochs. Each epoch involves going over all the inputs of the training set.\n",
    "        for epoch in range(epochs):\n",
    "            train = [a for a in zip(train_x_upd, train_y)]\n",
    "            random.shuffle(train)\n",
    "            batches = [train[k: k+batch_size] for k in range(0, train_x_upd.shape[0], batch_size)]\n",
    "            for batch in batches:\n",
    "                self.gradient_desc(batch, learn_rate)\n",
    "\n",
    "            evaluate = 0\n",
    "\n",
    "            for i in range(test_x_upd.shape[0]):\n",
    "                self.feedfwd(test_x_upd[i])\n",
    "                if(test_y[i] == self.outputlayer.argmax()): evaluate+=1\n",
    "            print(\"Epoch no. {0} done. Accuracy {1} pc\".format(epoch, evaluate/test_x.shape[0]*100))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 0 done. Accuracy 9.69 pc\n"
     ]
    }
   ],
   "source": [
    "net = Network(size_layers=[784, 20, 10])\n",
    "\n",
    "net.stochastic_batch(train_x_upd, train_y, 600, 30, 3000.0, test_x_upd, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.stochastic_batch(train_x_upd, train_y, 60, 30, 300.0, test_x_upd, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.stochastic_batch(train_x_upd, train_y, 10, 30, 30.0, test_x_upd, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.stochastic_batch(train_x_upd, train_y, 60, 30, 300.0, test_x_upd, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.stochastic_batch(train_x_upd, train_y, 10, 100, 90.0, test_x_upd, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"weights_inp_hid.csv\", net.weights_inp_hid, delimiter= ',')\n",
    "np.savetxt(\"weights_hid_out.csv\", net.weights_hid_out, delimiter= ',')\n",
    "np.savetxt(\"bias_hid.csv\", net.bias_hid, delimiter= ',')\n",
    "np.savetxt(\"bias_out.csv\", net.bias_out, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate = 0\n",
    "\n",
    "for i in range(train_x_upd.shape[0]):\n",
    "    net.feedfwd(train_x_upd[i])\n",
    "    if(train_y[i].argmax() == net.outputlayer.argmax()): evaluate+=1\n",
    "print(\"Accuracy {0} pc on training dataset\".format(evaluate/train_x_upd.shape[0]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
